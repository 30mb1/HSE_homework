{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Домашнее задание №1\n",
    "## Алиев М.А. группа 154\n",
    "### Тема - реализация графематического анализатор с помощью регулярных выражений\n",
    "\n",
    "#### Введение\n",
    "\n",
    "_Токенизация_ — это самый первый шаг при обработке текста. Заключается в разбиении (разделении) длинных строк текста в более мелкие: абзацы делим на предложения, предложения на слова. Токенизация обычно считается легкой относительно других задач на естественном языке и одной из наиболее неинтересных задач. Однако ошибки, сделанные на этом этапе, будут распространяться на более поздние этапы и вызывать проблемы, поэтому очень важно постараться допустить на этом шаге минимум промахов, а еще лучше не допустить вообще, однако, наврядли это возможно, учитывая, что мы анализируем естественный язык, который практически невозможно полностью систематизировать.\n",
    "\n",
    "На Python уже существует довольно неплохой пакет для NLP - NLTK, впрочем, качество его токенизации на русскоязычных текстах оставляет желать лучшего и с трудом превосходит результат обычного разбиения по пробелу. В своем токенизаторе в качестве инструмента я буду использовать регулярные выражения и \"инженерный\" подход. С помощью набора шаблонов я попытаюсь описать самые часто используемые сложные \"токены\" — телефонные номера, даты, аббревиатуры, имена, какие-то модели техники и т.д. Минус такого подхода в том, что невозможно создать универсальный шаблон — в зависимости от специфики текста могут потребоваться совсем другие правила разбиения: техническая статья может изобиловать сложными формулами и числами, медицинская — химическими формулами и сокращениями. В NLTK, например, есть отдельный токенизатор для Twitter. Конечно, не будем забывать и о варианте, использующем машинное обучение, однако для этого требуется большое количество размеченных текстов и машина с хорошой вычислительной мощностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определяем простые слова\n",
    "Начнем с самого просто - обычных слов, представляющих из себя последовательность букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Напиток', 'имел', 'терпко', 'сладкий', 'вкус', 'как', 'будто', 'был', 'сделан', 'из', 'мёда']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "word = \"(?:[А-Яа-яёЁ]+)\"\n",
    "print (re.findall(word, 'Напиток имел терпко-сладкий вкус, как-будто был сделан из мёда.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, все слова были корректно определены в токены, однако некоторые выражения, написанные через дефис, могут потерять часть смысла при разделении, поэтому добавим в правило использование дефиса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Напиток', 'имел', 'терпко-сладкий', 'вкус', 'как-будто', 'был', 'сделан', 'из', 'мёда']\n"
     ]
    }
   ],
   "source": [
    "# обычные слова и слова через дефис\n",
    "word = \"(?:(?:[А-Яа-яёЁ]+\\s?\\-\\s?[А-Яа-яёЁ]+)|(?:[А-Яа-яёЁ]+))\"\n",
    "print (re.findall(word, 'Напиток имел терпко-сладкий вкус, как-будто был сделан из мёда.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужный результат получен. Визуализируем полученное регулярное выражение. (зеленый цвет означает вхождение 0 или 1 раз)\n",
    "![words re](reg1.png)\n",
    "Далее, добавим возможность корректно определять разнообразные сокращения: и т.д., ч.т.д., т.е и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['т.д.', 'тд.', 'ч.т.д.', 'см.', 'г.', 'с.', 'кв.']\n"
     ]
    }
   ],
   "source": [
    "# сокращения - т.е, т.д, в. (век), с. (страница), г. (город), кв.(квартира), см. (сантиметр)\n",
    "# и т.д - то есть 1-2 буквы, заканчивающиеся точкой\n",
    "short = '(?:[а-я]{1,2}\\.(?:[а-я]{1,2}(?![а-я])\\.?)*)'\n",
    "\n",
    "print (re.findall(short, 'Много сокращений и т.д. или же тд., еще есть ч.т.д. или же и см., г., с. кв.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![abbrev re](reg7.png)\n",
    "Ну и конечно же нельзя забывать, что в тексте довольно часто могут встречаются имена (Ф.И.О), и если не добавить нужного правила, инициалы могут быть отделены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Л. Н. Толстой', 'Пушкин А.С']\n"
     ]
    }
   ],
   "source": [
    "# имена (Инициалы справа/слева)\n",
    "names = '(?:(?:[А-Я]\\.?\\s?[А-Я]\\.?\\s?)?[А-Я][а-я]+(?:\\s[А-Я]\\.\\s?[А-Я]\\.?)?)'\n",
    "\n",
    "print (re.findall(names, 'Л. Н. Толстой и Пушкин А.С - известные писатели'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация:\n",
    "![names re](reg8.png)\n",
    "Казалось бы, мы уже имеем минимально работающий токенизатор, однако если взять реальный текст, то можно убедиться, что этого недостаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ищем даты\n",
    "Откроем любую статью на википедии. Первое, что сразу бросается в глаза, это большое количество чисел, а также дат в самом разном формате. Вариантов записи крайне много, но, опять же, начнем с чего-то простого, а именно с со следующих форматов:\n",
    " - M/D/YY\n",
    " - M/D/YYYY\n",
    " - MM/DD/YY\n",
    " - MM/DD/YYYY\n",
    "\n",
    "Также учтем, что в качестве разделителя может быть использован не только '/', но и '.', а также '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1-2-1998', '02.04.2012', '01/12/12', '1.1.12']\n"
     ]
    }
   ],
   "source": [
    "# даты M/D/YY, MM/DD/YY, and MM/DD/YYYY с / или - или .\n",
    "dates = '(?:\\d{1,2}[-\\./]\\d{1,2}[-\\./](?:\\d{4}|\\d{2}))'\n",
    "\n",
    "# в этом регулярном выражении мы нацелены только на поиск дат, поэтому остальные токены будут проигнорированы\n",
    "print (re.findall(dates, '1-ая дата - 1-2-1998, 2-ая - 02.04.2012, 3-ья- 01/12/12, 4-ая - 1.1.12'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем:\n",
    "![dates re](reg2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим более простую запись вида YYYY. Найти ее намного проще, однако за ней очень часто следует сокращение \"г.\" или же разнообразные словоформы слова \"год\", без которых смысл числа теряется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1812 года', '1812 г.']\n"
     ]
    }
   ],
   "source": [
    "# распознает словоформы слова год, а также некоторые сокращения: \"г\" или же \"г.\"\n",
    "year = '(?:\\s?(?:год[а-я]*)|(?:г\\.)|(?:г(?![а-я])))'\n",
    "\n",
    "# получаем итоговое регулярное выражение для дат\n",
    "dates2 = '(?:\\d{1,4}\\s*(?:'+year+'))'\n",
    "print (re.findall(dates2, 'Отечественная война 1812 года — война между Российской и Французской империями на территории России в 1812 г.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усложним задачу, добавив более нестандартные случаи — временные отрезки вида YYYY-YYYY гг. с возможность добавления приставок до н.э. или н.э. Также можно заметить, что люди очень часто забывают ставить точки после сокращений или же добавляют/пропускают пробел, будем это учитывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1812—1814 гг.']\n",
      "['2012—2018 г.г', '100 г. до н. э.', '123—222 годах н.э']\n"
     ]
    }
   ],
   "source": [
    "# немного больше работы с датами YYYY г. (или же YYYY—YYYY гг. / г.г.) + до + н.э.\n",
    "dates2 = '(?:\\d{1,4}\\s*(?:(?:\\—\\s*\\d{1,4}\\s*(?:(?:г\\.\\s*г\\.?)|(?:гг\\.?)|(?:год[а-я]*))?)|'+year+')(?:\\s*(?:до )?н\\.?\\s?э\\.?)?)'\n",
    "print (re.findall(dates2, 'Вскоре после окончания кампаний 1812—1814 гг. полякам было предоставлено право вернуться на родину.'))\n",
    "\n",
    "# для проверки всех кейсов составим искусственную строку чисто из нужных нам дат\n",
    "print (re.findall(dates2, 'Какой-то промежуток: 2012—2018 г.г и еще один 100 г. до н. э.,  в 123—222 годах н.э '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация:\n",
    "![re dates2](reg3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И последний шаблон с датами, который я рассмотрю, это выражение вида <число> <месяц> <год> г., например \"30 августа 1814 года/г.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17 сентября 1939 года', '2 августа 123г.']\n"
     ]
    }
   ],
   "source": [
    "# чтобы случайно не принять за дату другую фразу, содержащую 2 числа, точно определим все\n",
    "# возможные варианты, а именно все 12 месяцев\n",
    "all_months = '(?:(?:[Яя]нваря)|(?:[Фф]евраля)|(?:[Мм]арта)|(?:[Аа]преля)|(?:[Мм]ая)|(?:[Ии]юня)|(?:[Ии]юля)|(?:[Аа]вгуста)|(?:[Сс]ентября)|(?:[Оо]ктября)|(?:[Нн]оября)|(?:[Дд]екабря))'\n",
    "\n",
    "# комплексные даты: 1 <месяца> YYYYг.\n",
    "dates3 = '(?:\\d{1,2}\\s?\\-?\\s?[а-я]{0,2}\\s+' + all_months + '\\s+\\d{1,4}\\s*'+year+'?)'\n",
    "print (re.findall(dates3, 'дата - 17 сентября 1939 года и еще дата 2 августа 123г.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация:\n",
    "![dates3 re](reg4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Числа, числительные, единицы измерения\n",
    "\n",
    "Очевидно, что никакой текст не обойдется без цифр, особенно технического толку. Создадим шаблон, распознающий любое действительное число + экспоненту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.13е+10', '-.2е-123', '0.123', '12333333']\n"
     ]
    }
   ],
   "source": [
    "# действительные числа, возможно в экспоненциальной записи + %\n",
    "nums = '(?:[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[еЕ][-+]?\\d+)?(?:\\s?%)?)'\n",
    "\n",
    "print (re.findall(nums, 'числаа 3.13е+10 и еще -.2е-123 а вот и еще 0.123 и 12333333'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, немного \"прокачаем\" наше регулярное выражение, добавив возможность распозновать находящиеся рядом единицы измерений (кг/м3, Дж/моль и тд.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12.23', '29999 Дж/К', '3.14Е12', '-0.3е-15 Кл/В']\n"
     ]
    }
   ],
   "source": [
    "# единица измерения: мг/кг, Дж/моль и т.д.\n",
    "measure = '(?:[А-Яа-яёЁ]+/[А-Яа-яёЁ]+)'\n",
    "\n",
    "# объединим вместе\n",
    "nums_measure = '(?:'+nums+'(?:\\s?'+measure+')?)'\n",
    "print (re.findall(nums_measure, '12.23 29999 Дж/К  3.14Е12 -0.3е-15 Кл/В'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация:\n",
    "![nums+measure re](reg5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и закончим довольно простым, но часто используемым случаем - числительными вида 30-е, 124-ый, 23-их и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31- ый', '3-го', '6 -го']\n"
     ]
    }
   ],
   "source": [
    "# числительные в числовой записи: 30-е, 12-ый и тд.\n",
    "numerals = '(?:\\d+\\s?\\-\\s?[а-я]{1,2})'\n",
    "\n",
    "# да, снова учитываются лишние пробелы\n",
    "print (re.findall(numerals, '31- ый батальон, 3-го дня 6 -го месяца'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![numerals re](reg6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Небольшой бонус\n",
    "В качестве некоторого дополнения, можно добавить шаблоны российских номеров телефонов (на случай разбора текстов газет) или же моделей техники/аббревиатур:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+79652238745', '8(495) 123 22 34', '7(876)123-11-23']\n"
     ]
    }
   ],
   "source": [
    "# российский телефонный номер\n",
    "tel = '(?:(?:(?:\\+7)|7|8)?[\\s\\-]?\\(?[489][0-9]{2}\\)?[\\s\\-]?[0-9]{3}[\\s\\-]?[0-9]{2}[\\s\\-]?[0-9]{2})'\n",
    "\n",
    "print (re.findall(tel, '+79652238745,,,8(495) 123 22 34...7(876)123-11-23'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tel re](reg9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['АК-74м', 'ТУ-154', 'ИЛ-2', 'БИЗОН-12-М2', 'ГОСТ-23-11']\n"
     ]
    }
   ],
   "source": [
    "# сокращения: модели техники / версии / аббревиатуры\n",
    "models = '(?:(?<!\\w)[А-Яа-яёЁ]+\\-?\\d+(?:\\-?(?:(?:[А-Яа-яёЁ]+)|(?:\\d+)))*)'\n",
    "\n",
    "print (re.findall(models, 'Автомат АК-74м, 23е-23 танк ТУ-154, ИЛ-2, БИЗОН-12-М2, ГОСТ-23-11'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![models re](reg10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итог\n",
    "\n",
    "Теперь, когда мы проверили работу каждого шаблона по отдельности, мы довольно просто можем их соединить. Однако учитывая особенности работы регулярных выражений, расположить их нужно в порядке убывания сложности. К примеру распознование дат должно идти до распознования чисел и сокращений, иначе токен \"1945 г.\" может быть преждевременно разобран на 2 части - число и сокращение, а мы пытаемся этого избежать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['В', '128 г.', 'слово', '30-е', '123-ий', 'Толстой Л.Н', '0-1-1222', 'году', '125—123 годах до н.э.', '201 г н.э', '1998 г.', '22.3.1998', '-2.234е-12 л/кг', 'дж/моль', 'луна', '7-ой', 'отряд', 'с.', '123', 'учебника', 'где', 'объясняются', 'и', 'тд.', 'главы', '12—23гг.', 'какой-то', 'ч.т.д', '-2.23е18', 'коровы', 'телефон', '8(987)2333212', 'КРН-23-1м']\n"
     ]
    }
   ],
   "source": [
    "# объединим все регулярные выражения в порядке убывания сложности\n",
    "all_regex = [tel, dates3, dates2, dates, numerals, nums_measure, measure, models, names, short, word]\n",
    "\n",
    "# нарядли получится найти рельный текст, с помощью которого получилось бы протестировать работу всех частей\n",
    "# (либо это должны быть достаточно большой текст) поэтому подадим на вход искусственный набор слов\n",
    "test_text = ('В 128 г. слово 30-е 123-ий Толстой Л.Н 0-1-1222 году 125—123 годах до н.э. 201 г н.э '\n",
    "             '1998 г. 22.3.1998 -2.234е-12 л/кг дж/моль луна 7-ой отряд с. 123 учебника, где объясняются и тд. '\n",
    "             'главы 12—23гг. какой-то ч.т.д -2.23е18 коровы телефон — 8(987)2333212 КРН-23-1м')\n",
    "print (re.findall('|'.join(all_regex), test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем свой модуль\n",
    "\n",
    "Для удобства использования логично собрать все функции и возможности и оформить в виде класса. Также добавим некоторых дополнительных возможностей, которые предоставляют и реальные токенизаторы.\n",
    "\n",
    "Реализованный класс называется ```Tokenizer```, его главной функцией конечно же является ```tokenize()``` - она собирает все сохраненные в объекте класса регулярные выражения в одно и проводит токенизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['В', '128 г.', 'слово', '30-е', '123-ий', 'Толстой Л.Н', '0-1-1222', 'году', '125—123 годах до н.э.', '201 г н.э', '1998 г.', '22.3.1998', '-2.234е-12 л/кг', 'дж/моль', 'луна', '7-ой', 'отряд', 'с.', '123', 'учебника', 'где', 'объясняются', 'и', 'тд.', 'главы', '12—23гг.', 'какой-то', 'ч.т.д', '-2.23е18', 'коровы', 'телефон', '8(987)2333212', 'КРН-23-1м']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tok = Tokenizer()\n",
    "print (tok.tokenize(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если мы хотим воспользоватся отдельным регулярным выражением и найти какие-то отдельные выражения? На этот случай для каждого регулярного выражения была добавлена функция, проводящая по тексту поиск с его использованием:\n",
    " - find_dates()\n",
    " - find_nums()\n",
    " - find_names()\n",
    " - find_measure()\n",
    " - find_models()\n",
    " - find_nums_measure()\n",
    " - find_short()\n",
    " - find_abbrev()\n",
    " - find_tel()\n",
    " - find_word()\n",
    "\n",
    "Описание каждого из регулярных выражений находится выше в отчете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['128 г.', '0-1-1222', '125—123 годах до н.э.', '201 г н.э', '1998 г.', '22.3.1998', '12—23гг.']\n",
      "['Толстой Л.Н']\n"
     ]
    }
   ],
   "source": [
    "# Найдем все даты\n",
    "print (tok.find_dates(test_text))\n",
    "\n",
    "# теперь попробуем найти имена\n",
    "print (tok.find_names(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не будем забывать, что, как я говорил в начале, каждый текст имеет свою специфику, поэтому важно иметь возможность \"подогнать\" токенизатор под свои нужды. На этот случай есть функция ```register_regex()```. Она позволяет добавить в набор шаблонов свой собственный, а также зарегестрировать соответствующую ```find_{}``` функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Текст', 'со', 'смайликами']\n",
      "['Текст', 'со', 'смайликами', ';>)']\n",
      "[';>)']\n"
     ]
    }
   ],
   "source": [
    "# Допустим что мы решили проанализировать тексты социальных сетей, насыщенные смайликами\n",
    "# смайлик не будет обнаржуен\n",
    "print (tok.tokenize('Текст со смайликами ;>)'))\n",
    "\n",
    "# зарегестрируем новое регулярное выражение\n",
    "tok.register_regex('(?:\\;\\>\\))', 'smiles')\n",
    "# теперь смайл корректно распознан\n",
    "print (tok.tokenize('Текст со смайликами ;>)'))\n",
    "# также можно провести поиск чисто на смайл\n",
    "print (tok.find_smiles('Текст со смайликами ;>)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всегда можно выгрузить все используемые модели с помощью функции ```get_models()``` (функция вернет словарь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure (?:[А-Яа-яёЁ]+/[А-Яа-яёЁ]+)\n",
      "short (?:[а-я]{1,2}\\.(?:[а-я]{1,2}(?![а-я])\\.?)*)\n",
      "models (?:(?<!\\w)[А-Яа-яёЁ]+\\-?\\d+(?:\\-?(?:(?:[А-Яа-яёЁ]+)|(?:\\d+)))*)\n",
      "tel (?:(?:(?:\\+7)|7|8)?[\\s\\-]?\\(?[489][0-9]{2}\\)?[\\s\\-]?[0-9]{3}[\\s\\-]?[0-9]{2}[\\s\\-]?[0-9]{2})\n",
      "dates (?:\\d{1,2}\\s?\\-?\\s?[а-я]{0,2}\\s+(?:(?:[Яя]нваря)|(?:[Фф]евраля)|(?:[Мм]арта)|(?:[Аа]преля)|(?:[Мм]ая)|(?:[Ии]юня)|(?:[Ии]юля)|(?:[Аа]вгуста)|(?:[Сс]ентября)|(?:[Оо]ктября)|(?:[Нн]оября)|(?:[Дд]екабря))\\s+\\d{1,4}\\s*(?:\\s?(?:год[а-я]*)|(?:г\\.)|(?:г(?![а-я])))?)|(?:\\d{1,4}\\s*(?:(?:\\—\\s*\\d{1,4}\\s*(?:(?:г\\.\\s*г\\.?)|(?:гг\\.?)|(?:год[а-я]*))?)|(?:\\s?(?:год[а-я]*)|(?:г\\.)|(?:г(?![а-я]))))(?:\\s*(?:до )?н\\.?\\s?э\\.?)?)|(?:\\d{1,2}[-\\./]\\d{1,2}[-\\./](?:\\d{4}|\\d{2}))\n",
      "nums (?:[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[еЕ][-+]?\\d+)?(?:\\s?%)?)\n",
      "word (?:(?:[А-Яа-яёЁ]+\\s?\\-\\s?[А-Яа-яёЁ]+)|(?:[А-Яа-яёЁ]+))\n",
      "names (?:(?:[А-Я]\\.?\\s?[А-Я]\\.?\\s?)?[А-Я][а-я]+(?:\\s[А-Я]\\.\\s?[А-Я]\\.?)?)\n",
      "smiles (?:\\;\\>\\))\n",
      "nums_measure (?:(?:[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[еE][-+]?\\d+)?(?:\\s?%)?)(?:\\s?(?:[А-Яа-яёЁ]+/[А-Яа-яёЁ]+))?)\n",
      "numerals (?:\\d+\\s?\\-\\s?[а-я]{1,2})\n"
     ]
    }
   ],
   "source": [
    "for key, value in tok.get_models().items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И последней функцией является возможность добавления словаря мульти-токенов. Можно загрузить списки слов, которые должны быть объединены в единый токен через какой-то разделитель, если идут рядом с помощью функции ```add_multi_word()``` и затем подать на вход список токенов, где должна пройти проверка на вхождение этих случаев - ```multi_word_tokenize()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кое-кто', 'купил', 'кое-что']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# эти комбинации слов должны быть объединены через '-', если идут подряд\n",
    "tok.add_multi_word([('кое', 'что'), ('кое', 'кто')], '-')\n",
    "\n",
    "# токены из списка были объединены\n",
    "tok.multi_word_tokenize(['кое','кто','купил','кое','что'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и немного тестов на реальных кейсах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 августа 1939 года',\n",
       " 'после',\n",
       " 'срыва',\n",
       " 'Московских',\n",
       " 'переговоров',\n",
       " 'о',\n",
       " 'создании',\n",
       " 'коалиции',\n",
       " 'СССР',\n",
       " 'с',\n",
       " 'Англией',\n",
       " 'и',\n",
       " 'Францией',\n",
       " 'Германия',\n",
       " 'и',\n",
       " 'СССР',\n",
       " 'заключили',\n",
       " 'пакт',\n",
       " 'о',\n",
       " 'ненападении',\n",
       " 'что',\n",
       " 'ослабило',\n",
       " 'Антикоминтерновский',\n",
       " 'пакт',\n",
       " 'привело',\n",
       " 'к',\n",
       " 'охлаждению',\n",
       " 'отношений',\n",
       " 'между',\n",
       " 'Германией',\n",
       " 'и',\n",
       " 'Японией',\n",
       " 'и',\n",
       " '13.04.1941',\n",
       " 'г.',\n",
       " 'был',\n",
       " 'заключен',\n",
       " 'советско- японский',\n",
       " 'договор',\n",
       " 'о',\n",
       " 'нейтралитете']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# историческая статья с википедии\n",
    "tok.tokenize(('23 августа 1939 года после срыва Московских переговоров о'\n",
    "              ' создании коалиции СССР с Англией и Францией, Германия и '\n",
    "              'СССР заключили пакт о ненападении, что ослабило Антикомин'\n",
    "              'терновский пакт, привело к охлаждению отношений между Гер'\n",
    "              'манией и Японией и 13.04.1941г. был заключен советско- япо'\n",
    "              'нский договор о нейтралитете.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Этот',\n",
       " 'мудреный',\n",
       " 'и',\n",
       " 'хлопотливый',\n",
       " 'случай',\n",
       " 'как',\n",
       " 'выражался',\n",
       " 'сам',\n",
       " 'Тоцкий',\n",
       " 'начался',\n",
       " 'очень',\n",
       " 'давно',\n",
       " 'лет',\n",
       " 'восемнадцать',\n",
       " 'этак',\n",
       " 'назад',\n",
       " 'Рядом',\n",
       " 'с',\n",
       " 'однимиз',\n",
       " 'богатейших',\n",
       " 'поместий',\n",
       " 'Афанасия',\n",
       " 'Ивановича',\n",
       " 'в',\n",
       " 'одной',\n",
       " 'из',\n",
       " 'срединныхгуберний',\n",
       " 'бедствовал',\n",
       " 'один',\n",
       " 'мелкопоместный',\n",
       " 'и',\n",
       " 'беднейший',\n",
       " 'помещик',\n",
       " 'Это',\n",
       " 'был',\n",
       " 'человек',\n",
       " 'замечательный',\n",
       " 'по',\n",
       " 'своим',\n",
       " 'беспрерывным',\n",
       " 'и',\n",
       " 'анекдотическим',\n",
       " 'неудачам',\n",
       " 'один',\n",
       " 'отставной',\n",
       " 'офицер',\n",
       " 'хорошей',\n",
       " 'дворянской',\n",
       " 'фамилии',\n",
       " 'и',\n",
       " 'даже',\n",
       " 'в',\n",
       " 'этом',\n",
       " 'отношении',\n",
       " 'почище',\n",
       " 'Тоцкого',\n",
       " 'некто',\n",
       " 'Филипп',\n",
       " 'Александрович',\n",
       " 'Барашков',\n",
       " 'Весь',\n",
       " 'задолжавшийся',\n",
       " 'и',\n",
       " 'заложившийся',\n",
       " 'он',\n",
       " 'успел',\n",
       " 'уже',\n",
       " 'наконец',\n",
       " 'после',\n",
       " 'каторжных',\n",
       " 'почти',\n",
       " 'мужичьих',\n",
       " 'трудов',\n",
       " 'устроить',\n",
       " 'кое-как',\n",
       " 'свое',\n",
       " 'маленькое',\n",
       " 'хозяйство',\n",
       " 'удовлетворительно',\n",
       " 'Ф. М. Достоевский',\n",
       " 'Идиот',\n",
       " '1869 г.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# немного классики\n",
    "tok.tokenize(('Этот мудреный и хлопотливый «случай» (как выражался сам Тоцкий)'\n",
    "              'начался очень давно, лет восемнадцать этак назад. Рядом с одним'\n",
    "              'из богатейших поместий Афанасия Ивановича, в одной из срединных'\n",
    "              'губерний, бедствовал один мелкопоместный и беднейший помещик. Э'\n",
    "              'то был человек замечательный по своим беспрерывным и анекдотиче'\n",
    "              'ским неудачам, – один отставной офицер, хорошей дворянской фами'\n",
    "              'лии, и даже в этом отношении почище Тоцкого, некто Филипп Алекс'\n",
    "              'андрович Барашков. Весь задолжавшийся и заложившийся, он успел '\n",
    "              'уже наконец после каторжных, почти мужичьих трудов устроить кое'\n",
    "              '-как свое маленькое хозяйство удовлетворительно». Ф. М. Дос'\n",
    "              'тоевский «Идиот», 1869 г.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.602176565е-5', '100000', '1.602176565']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# попробуем распарсить формулу\n",
    "tok.tokenize('1.602176565е-5 * 100000 = 1.602176565')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
