{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание #2\n",
    "## Выполнили студенты 154 группы\n",
    "### Алиев М.А. Потехин С.А. Кугушева А.С."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score\n",
    "from tqdm import tqdm\n",
    "from plotly import tools\n",
    "\n",
    "notebook_start = time()\n",
    "# multiprocessing.set_start_method('forkserver')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML файл обработан с помощью регулярных выражений, задача сведена к бинарной путем скрещивания классов 0 и 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(name):\n",
    "    with open(name, 'r') as f:\n",
    "        content = f.read()\n",
    "    phrases = re.findall('(?<=\"text_[1,2]\">)(.*)(?=</value>)', content)\n",
    "    classes = re.findall('(?<=\"class\">)(.*)(?=</value>)', content)\n",
    "    def to_binary(elem):\n",
    "        if elem == '-1': return 0\n",
    "        if elem == '0':return 1\n",
    "        return 1\n",
    "    classes = [to_binary(i) for i in classes]\n",
    "    return (phrases, classes)\n",
    "\n",
    "# https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases, classes = parse_xml('paraphrases.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве препроцессинга текста была проведена лемматизация всех слов с помощью инструмента Mystem от яндекса. Далее лемматизированный текст был токенизирован и избавлен от набора стопcлов, взятого из пакета NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# we need to lemmatize and tokenize text before\n",
    "# teaching W2V model with them\n",
    "m = Mystem()\n",
    "tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "tokenized_corpus = []\n",
    "\n",
    "def lemmatize_text(text, tok=True):\n",
    "    lemmatized = ''.join(m.lemmatize(text))\n",
    "    tokens = [i for i in tokenizer(lemmatized) if i not in stopwords.words('russian')]\n",
    "    if not tok:\n",
    "        tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "for i in phrases:\n",
    "    tokenized_corpus.append(lemmatize_text(i))\n",
    "    \n",
    "pairs = list(chunks(tokenized_corpus, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения без учителя было решено использовать модель эмбеддингов *Word2vec* с размерностью 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# creating and teaching model\n",
    "model_w2v = Word2Vec(tokenized_corpus, size=100, workers=4, iter=25, min_count=2)\n",
    "# extracting words vectors from word2vec\n",
    "w2v = dict(zip(model_w2v.wv.index2word, model_w2v.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой пары размеченных парафраз посчитаем их Word Mover's Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = []\n",
    "for i in pairs:\n",
    "    dist = model_w2v.wmdistance(i[0], i[1])\n",
    "    if dist == float('inf'): dist = 10\n",
    "    all_distances.append(dist)\n",
    "    \n",
    "all_distances = np.array(all_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подберем порог WMD, который будет определять, явлются ли 2 предложения парафразами. Для этого разобъем выборку на обучающую и тестовую в соотношении 3 к 1 и на первой будем перебирать значения порога в интервале от 5го до 95го процентиля. Итоговым порогом возьмем тот, для которого f1-score будет максимальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:08<00:00, 555.68it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAHwCAYAAACG+PhNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa32e6052e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# kf = KFold(n_splits=10, shuffle=True)\n",
    "X, y = all_distances, np.array(classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=224)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "best_thresholds = []\n",
    "res_f1 = []\n",
    "min_thr, max_thr = np.percentile(all_distances, [5, 95])\n",
    "threshold_list = np.linspace(min_thr, max_thr, num=5000)\n",
    "for thr in tqdm(threshold_list):\n",
    "    y_pred = np.ones(len(X_train))\n",
    "    y_pred[np.where(X_train > thr)[0]] = 0\n",
    "    res_f1.append(f1_score(y_pred, y_train))\n",
    "best_threshold = threshold_list[np.argmax(res_f1)]\n",
    "plt.plot(threshold_list, res_f1)\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('Threshold value & F1-score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7493082457111234\n",
      "F-Score: 0.8275599543205177\n",
      "Macro-score: 0.769721002690837\n"
     ]
    }
   ],
   "source": [
    "final_thr = best_threshold\n",
    "y_pred = np.ones(len(X_test))\n",
    "y_pred[np.where(X_test > best_threshold)[0]] = 0\n",
    "\n",
    "print ('Accuracy score: {}'.format(accuracy_score(y_pred, y_test)))\n",
    "print ('F-Score: {}'.format(f1_score(y_pred, y_test)))\n",
    "print ('Macro-score: {}'.format(fbeta_score(y_pred, y_test, beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения с учителем кроме уже использованного усредненного Word2Vec возьмем модели эмбеддингов Doc2Vec, FastText, Glove, а также Word2Vec с Tf-Idf весами. На каждом из них обучим kNN, MLP, RandomForest, SVM, LogReg, GradientBoosting и сравним результаты с помощью метрик accuracy, f1-score, fbeta-score(macro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import FastText\n",
    "from glove import Glove, Corpus\n",
    "\n",
    "# we already have word2vec\n",
    "# doc2vec\n",
    "documents_train = [TaggedDocument(' '.join(text), [id_]) for id_, text in enumerate(tokenized_corpus)]\n",
    "model_d2v = Doc2Vec(documents_train, vector_size=100, workers=4, epochs=25, min_count=2)\n",
    "\n",
    "# fastText\n",
    "model_ft = FastText(tokenized_corpus, size=100, workers=4, iter=25, min_count=2)\n",
    "# extracting words vectors from fastText\n",
    "ft = dict(zip(model_ft.wv.index2word, model_ft.wv.vectors))\n",
    "\n",
    "# Glove\n",
    "corpus = Corpus()\n",
    "corpus.fit(tokenized_corpus, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.01)\n",
    "glove.fit(corpus.matrix, epochs=25, no_threads=4)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "# extracting words vectors\n",
    "glv = dict(zip(list(glove.dictionary.keys()), glove.word_vectors[list(glove.dictionary.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word_dict):\n",
    "        self.word_dict = word_dict\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word_dict[w] for w in words if w in self.word_dict] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "class D2VEmbeddingVectorizer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.infer_vector(\" \".join(words), epochs=5) for words in X])\n",
    "    \n",
    "# same as MeanEmbeddingVectorizer but adding weights using tf-idf\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word_dict):\n",
    "        self.word_dict = word_dict\n",
    "        self.word2weight = None\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.word2weight:\n",
    "            return self\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word_dict[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word_dict] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые модели, такие как деревья или градиентный бустинг, требуют подбора параметров/гиперпараметров, для оптимизации процесса обучения или для предотвращения переобучения. Для перебора вариантов используем RandomizedSearchCV и проверим случайные 30 вариаций из заготовленного сета настроек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, GradientBoostingClassifier as GBC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "forest_params = {\n",
    "    'n_estimators' : np.linspace(100, 1000, 10, dtype=np.int64),\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 2, 4],\n",
    "    'max_depth' : list(np.linspace(10, 110, num = 11, dtype=np.int64)) + [None],\n",
    "    'bootstrap' : [True, False],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C' : np.logspace(-3, 4, num=10),\n",
    "    'gamma' : list(np.logspace(-6, 1, num=10)) + ['auto'],\n",
    "    'class_weight' : ['balanced', None]\n",
    "}\n",
    "\n",
    "boosting_params = {\n",
    "    'n_estimators' : np.linspace(50, 300, 10, dtype=np.int64),\n",
    "    'max_depth' : np.linspace(3, 11, 5, dtype=np.int64),\n",
    "    'min_samples_split' : [2, 4, 6],\n",
    "    'min_samples_leaf' : [1, 2, 4],\n",
    "    'learning_rate' : np.logspace(-2, 0, num=10),\n",
    "    'max_features' : np.linspace(5, 30, 6, dtype=np.int64)\n",
    "}\n",
    "\n",
    "vectorizers = [\n",
    "        ('Word2vec embedding', MeanEmbeddingVectorizer(w2v)),\n",
    "        ('Glove embedding', MeanEmbeddingVectorizer(glv)),\n",
    "        ('FastText embedding', MeanEmbeddingVectorizer(ft)),\n",
    "        ('Doc2vec embedding', D2VEmbeddingVectorizer(model_d2v)),\n",
    "        ('Weighted-word2vec embedding', TfidfEmbeddingVectorizer(w2v))\n",
    "    ]\n",
    "search_iterations = 30\n",
    "models = [\n",
    "        ('MultiLayerPerceptron', MLPClassifier(solver='adam', max_iter=300)),\n",
    "        ('LogisticRegression', LogisticRegression(solver='sag', n_jobs=-1)),\n",
    "        ('kNN', KNeighborsClassifier(n_jobs=-1)),\n",
    "        ('RandomForest', RandomizedSearchCV(RFC(n_jobs=-1), forest_params, n_iter=search_iterations)),\n",
    "        ('SVM', RandomizedSearchCV(SVC(), svm_params, n_iter=search_iterations)),\n",
    "        ('GradientBoosting', RandomizedSearchCV(GBC(), boosting_params, n_iter=search_iterations))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 0 ns, total: 12.2 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(pairs, np.array(classes), test_size=0.25, random_state=224)\n",
    "\n",
    "X_train_concat = [i[0] + i[1] for i in X_train]\n",
    "X_test_concat = [i[0] + i[1] for i in X_test]\n",
    "\n",
    "X_train_full, X_test_full = [], []\n",
    "for i in X_train:\n",
    "    X_train_full += [i[0], i[1]]\n",
    "    \n",
    "for i in X_test:\n",
    "    X_test_full += [i[0], i[1]]\n",
    "    \n",
    "X_full = X_train_full + X_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultiLayerPerceptron on Word2vec embedding\n",
      "Training MultiLayerPerceptron on Glove embedding\n",
      "Training MultiLayerPerceptron on FastText embedding\n",
      "Training MultiLayerPerceptron on Doc2vec embedding\n",
      "Training MultiLayerPerceptron on Weighted-word2vec embedding\n",
      "Training LogisticRegression on Word2vec embedding\n",
      "Training LogisticRegression on Glove embedding\n",
      "Training LogisticRegression on FastText embedding\n",
      "Training LogisticRegression on Doc2vec embedding\n",
      "Training LogisticRegression on Weighted-word2vec embedding\n",
      "Training kNN on Word2vec embedding\n",
      "Training kNN on Glove embedding\n",
      "Training kNN on FastText embedding\n",
      "Training kNN on Doc2vec embedding\n",
      "Training kNN on Weighted-word2vec embedding\n",
      "Training RandomForest on Word2vec embedding\n",
      "Training RandomForest on Glove embedding\n",
      "Training RandomForest on FastText embedding\n",
      "Training RandomForest on Doc2vec embedding\n",
      "Training RandomForest on Weighted-word2vec embedding\n",
      "Training SVM on Word2vec embedding\n",
      "Training SVM on Glove embedding\n",
      "Training SVM on FastText embedding\n",
      "Training SVM on Doc2vec embedding\n",
      "Training SVM on Weighted-word2vec embedding\n",
      "Training GradientBoosting on Word2vec embedding\n",
      "Training GradientBoosting on Glove embedding\n",
      "Training GradientBoosting on FastText embedding\n",
      "Training GradientBoosting on Doc2vec embedding\n",
      "Training GradientBoosting on Weighted-word2vec embedding\n",
      "CPU times: user 8h 26min 10s, sys: 6min 32s, total: 8h 32min 43s\n",
      "Wall time: 3h 19min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, macro, f1 = [], [], []\n",
    "for model in models:\n",
    "    for vect in vectorizers:\n",
    "        print ('Training {} on {}'.format(model[0], vect[0]))\n",
    "        p = Pipeline([vect, model])\n",
    "        p.fit(X_train_concat, y_train)\n",
    "        res = p.predict(X_test_concat)\n",
    "        accuracy.append(accuracy_score(res, y_test))\n",
    "        macro.append(fbeta_score(res, y_test, average='macro', beta=2))\n",
    "        f1.append(f1_score(res, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем данные. Для каждой модели эмбеддингов построим график с результатами всех методов и набора метрик на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "[ (3,1) x5,y5 ]  [ (3,2) x6,y6 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~30mb1/32.embed\" height=\"1000px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "embeddings = ('Word2Vec', 'Glove', 'FastText', 'Doc2Vec', 'WeightedWord2Vec')\n",
    "models_names = ('MLP','LogReg','kNN','RndFrst','SVM','GrBst')\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=2, subplot_titles=embeddings)\n",
    "\n",
    "ac_counter, f1_counter, fb_counter = 0, 0, 0\n",
    "all_models = []\n",
    "all_acc = defaultdict(lambda: [])\n",
    "all_f1 = defaultdict(lambda: [])\n",
    "all_fb = defaultdict(lambda: [])\n",
    "for model in models:\n",
    "    for vect in vectorizers:\n",
    "        all_models.append(model[0])\n",
    "        vect = vect[0].split()[0]\n",
    "        all_acc[vect].append(accuracy[ac_counter])\n",
    "        ac_counter += 1\n",
    "        all_f1[vect].append(f1[f1_counter])\n",
    "        f1_counter += 1\n",
    "        all_fb[vect].append(macro[fb_counter])\n",
    "        fb_counter += 1\n",
    "        \n",
    "show_ = False\n",
    "for idx, key in enumerate(all_acc.keys()):\n",
    "    if idx == 4: show_ = True\n",
    "    tmp = go.Scatter(\n",
    "        x=models_names,\n",
    "        y=all_acc[key],\n",
    "        name='Accuracy score',\n",
    "        line=dict(\n",
    "            color=('orange')\n",
    "        ),\n",
    "        showlegend=show_\n",
    "    )\n",
    "    tmp2 = go.Scatter(\n",
    "        x=models_names,\n",
    "        y=all_f1[key],\n",
    "        name='F1-score',\n",
    "        line=dict(\n",
    "            color=('blue')\n",
    "        ),\n",
    "        showlegend=show_\n",
    "    )\n",
    "    tmp3 = go.Scatter(\n",
    "        x=models_names,\n",
    "        y=all_fb[key],\n",
    "        name='Macro-score',\n",
    "        line=dict(\n",
    "            color=('red')\n",
    "        ),\n",
    "        showlegend=show_\n",
    "    )\n",
    "    if idx == 4: idx += 1\n",
    "    fig.append_trace(tmp, int((idx+1)/3 + 1), (idx+1)%2 + 1)\n",
    "    fig.append_trace(tmp2, int((idx+1)/3 + 1), (idx+1)%2 + 1)\n",
    "    fig.append_trace(tmp3, int((idx+1)/3 + 1), (idx+1)%2 + 1)\n",
    "\n",
    "fig['layout'].update(height=1000, width=1000, title='Embeddings & Scores')\n",
    "py.iplot(fig, filename='Embeddings subplots')\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка, если график не прогружается](https://plot.ly/~30mb1/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('acc.pickle', 'wb') as f:\n",
    "#     pickle.dump(accuracy, f)\n",
    "    \n",
    "# with open('f1.pickle', 'wb') as f:\n",
    "#     pickle.dump(f1, f)\n",
    "    \n",
    "# with open('f_ma.pickle', 'wb') as f:\n",
    "#     pickle.dump(macro, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "accuracy, macro, f1 = [], [], []\n",
    "with open('acc.pickle', 'rb') as f:\n",
    "    accuracy = pickle.load(f)\n",
    "    \n",
    "with open('f1.pickle', 'rb') as f:\n",
    "    f1 = pickle.load(f)\n",
    "    \n",
    "with open('f_ma.pickle', 'rb') as f:\n",
    "    macro = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Было реализовано несколько вариантов нейронных сетей: Siamese DAN (от предложенной реализации отличается тем, что в кач-ве функции активации была использована сигмоида) а также несколько более простых FNNs. Для реализации всех сетей был использован фреймворк Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из вариаций использует специальный слой Embedding, данные для которого нужно подготовить. Каждая пара предложений соединяется в одно и кодируется (для каждого слова уникальное число). Слой настраивается с помощью матрицы, создающейся на основе w2v модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Dropout, SpatialDropout1D, Flatten, Embedding\n",
    "from keras.layers import Convolution1D, GlobalMaxPool1D, Input, Add, Multiply, Average\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "X_full_concat = X_train_concat + X_test_concat\n",
    "\n",
    "max_length = max([len(i) for i in X_full_concat])\n",
    "\n",
    "# preparing data for special Embedding layer\n",
    "clear_texts = [' '.join(i) for i in X_full_concat]\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(clear_texts)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "clear_texts_train = [' '.join(i) for i in X_train_concat]\n",
    "clear_texts_test = [' '.join(i) for i in X_test_concat]\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs_train = t.texts_to_sequences(clear_texts_train)\n",
    "padded_texts_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n",
    "\n",
    "encoded_docs_test = t.texts_to_sequences(clear_texts_test)\n",
    "padded_texts_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "\n",
    "device_lib.list_local_devices()\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготавливаем данные - усредненный w2v вектор для каждого из предложений, которые пойдут на вход отдельным веткам нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 455 ms, sys: 3.99 ms, total: 459 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preparing data for siamese and standard fNN\n",
    "v = MeanEmbeddingVectorizer(w2v)\n",
    "# v.fit(X_train_full, y_train)\n",
    "embedded_data = v.transform(X_train_concat)\n",
    "embedded_data_test = v.transform(X_test_concat)\n",
    "\n",
    "texts_1, texts_2 = v.transform([i[0] for i in X_train]), v.transform([i[1] for i in X_train])\n",
    "texts_1_test, texts_2_test = v.transform([i[0] for i in X_test]), v.transform([i[1] for i in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         206848      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2048)         206848      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         4196352     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         4196352     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 2048)         0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         2098176     average_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            1025        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,955,201\n",
      "Trainable params: 11,955,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5420/5420 [==============================] - 4s 765us/step - loss: 0.6603 - acc: 0.6363\n",
      "Epoch 2/200\n",
      "5420/5420 [==============================] - 2s 317us/step - loss: 0.6245 - acc: 0.6683\n",
      "Epoch 3/200\n",
      "5420/5420 [==============================] - 2s 310us/step - loss: 0.6322 - acc: 0.6668\n",
      "Epoch 4/200\n",
      "5420/5420 [==============================] - 2s 321us/step - loss: 0.6080 - acc: 0.6790\n",
      "Epoch 5/200\n",
      "5420/5420 [==============================] - 2s 311us/step - loss: 0.5916 - acc: 0.6893\n",
      "Epoch 6/200\n",
      "5420/5420 [==============================] - 2s 302us/step - loss: 0.5780 - acc: 0.7007\n",
      "Epoch 7/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.5669 - acc: 0.7113\n",
      "Epoch 8/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.5635 - acc: 0.7105\n",
      "Epoch 9/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.5505 - acc: 0.7229\n",
      "Epoch 10/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.5379 - acc: 0.7304\n",
      "Epoch 11/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.5277 - acc: 0.7446\n",
      "Epoch 12/200\n",
      "5420/5420 [==============================] - 2s 323us/step - loss: 0.5099 - acc: 0.7496\n",
      "Epoch 13/200\n",
      "5420/5420 [==============================] - 2s 312us/step - loss: 0.5025 - acc: 0.7579\n",
      "Epoch 14/200\n",
      "5420/5420 [==============================] - 2s 318us/step - loss: 0.4896 - acc: 0.7686\n",
      "Epoch 15/200\n",
      "5420/5420 [==============================] - 2s 322us/step - loss: 0.4702 - acc: 0.7790\n",
      "Epoch 16/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.4715 - acc: 0.7755\n",
      "Epoch 17/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.4569 - acc: 0.7880\n",
      "Epoch 18/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.4351 - acc: 0.7983\n",
      "Epoch 19/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.4274 - acc: 0.7993\n",
      "Epoch 20/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.4302 - acc: 0.7987\n",
      "Epoch 21/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.4208 - acc: 0.8055\n",
      "Epoch 22/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.4138 - acc: 0.8101\n",
      "Epoch 23/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.3934 - acc: 0.8245\n",
      "Epoch 24/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.3841 - acc: 0.8277\n",
      "Epoch 25/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.3711 - acc: 0.8373\n",
      "Epoch 26/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.3789 - acc: 0.8216\n",
      "Epoch 27/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.3618 - acc: 0.8393\n",
      "Epoch 28/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.3647 - acc: 0.8369\n",
      "Epoch 29/200\n",
      "5420/5420 [==============================] - 2s 300us/step - loss: 0.3642 - acc: 0.8351\n",
      "Epoch 30/200\n",
      "5420/5420 [==============================] - 2s 324us/step - loss: 0.3349 - acc: 0.8515\n",
      "Epoch 31/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.3215 - acc: 0.8605\n",
      "Epoch 32/200\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.3243 - acc: 0.8554\n",
      "Epoch 33/200\n",
      "5420/5420 [==============================] - 2s 307us/step - loss: 0.3075 - acc: 0.8609\n",
      "Epoch 34/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.3031 - acc: 0.8664\n",
      "Epoch 35/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.2867 - acc: 0.8729\n",
      "Epoch 36/200\n",
      "5420/5420 [==============================] - 2s 318us/step - loss: 0.2791 - acc: 0.8788\n",
      "Epoch 37/200\n",
      "5420/5420 [==============================] - 2s 317us/step - loss: 0.2761 - acc: 0.8808\n",
      "Epoch 38/200\n",
      "5420/5420 [==============================] - 2s 308us/step - loss: 0.2691 - acc: 0.8827\n",
      "Epoch 39/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.2721 - acc: 0.8838\n",
      "Epoch 40/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.2587 - acc: 0.8886\n",
      "Epoch 41/200\n",
      "5420/5420 [==============================] - 2s 307us/step - loss: 0.2466 - acc: 0.8908\n",
      "Epoch 42/200\n",
      "5420/5420 [==============================] - 2s 313us/step - loss: 0.2619 - acc: 0.8880\n",
      "Epoch 43/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.2467 - acc: 0.8935\n",
      "Epoch 44/200\n",
      "5420/5420 [==============================] - 2s 309us/step - loss: 0.2439 - acc: 0.8902\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5420/5420 [==============================] - 2s 321us/step - loss: 0.2217 - acc: 0.9026\n",
      "Epoch 46/200\n",
      "5420/5420 [==============================] - 2s 319us/step - loss: 0.2130 - acc: 0.9076\n",
      "Epoch 47/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.2093 - acc: 0.9122\n",
      "Epoch 48/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.2123 - acc: 0.9090\n",
      "Epoch 49/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.2187 - acc: 0.9068\n",
      "Epoch 50/200\n",
      "5420/5420 [==============================] - 2s 320us/step - loss: 0.2009 - acc: 0.9161\n",
      "Epoch 51/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.1924 - acc: 0.9144\n",
      "Epoch 52/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.1925 - acc: 0.9183\n",
      "Epoch 53/200\n",
      "5420/5420 [==============================] - 2s 319us/step - loss: 0.1913 - acc: 0.9166\n",
      "Epoch 54/200\n",
      "5420/5420 [==============================] - 2s 314us/step - loss: 0.1890 - acc: 0.9183\n",
      "Epoch 55/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1736 - acc: 0.9247\n",
      "Epoch 56/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1747 - acc: 0.9256\n",
      "Epoch 57/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1642 - acc: 0.9332\n",
      "Epoch 58/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1717 - acc: 0.9284\n",
      "Epoch 59/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.1696 - acc: 0.9317\n",
      "Epoch 60/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.1573 - acc: 0.9356\n",
      "Epoch 61/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.1472 - acc: 0.9402\n",
      "Epoch 62/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.1463 - acc: 0.9373\n",
      "Epoch 63/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.1483 - acc: 0.9369\n",
      "Epoch 64/200\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.1437 - acc: 0.9441\n",
      "Epoch 65/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.1424 - acc: 0.9423\n",
      "Epoch 66/200\n",
      "5420/5420 [==============================] - 2s 312us/step - loss: 0.1459 - acc: 0.9417\n",
      "Epoch 67/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.1396 - acc: 0.9411\n",
      "Epoch 68/200\n",
      "5420/5420 [==============================] - 2s 317us/step - loss: 0.1400 - acc: 0.9423\n",
      "Epoch 69/200\n",
      "5420/5420 [==============================] - 2s 317us/step - loss: 0.1410 - acc: 0.9432\n",
      "Epoch 70/200\n",
      "5420/5420 [==============================] - 2s 323us/step - loss: 0.1383 - acc: 0.9432\n",
      "Epoch 71/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.1325 - acc: 0.9450\n",
      "Epoch 72/200\n",
      "5420/5420 [==============================] - 2s 310us/step - loss: 0.1340 - acc: 0.9399\n",
      "Epoch 73/200\n",
      "5420/5420 [==============================] - 2s 300us/step - loss: 0.1331 - acc: 0.9445\n",
      "Epoch 74/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1272 - acc: 0.9506\n",
      "Epoch 75/200\n",
      "5420/5420 [==============================] - 2s 306us/step - loss: 0.1228 - acc: 0.9524\n",
      "Epoch 76/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1213 - acc: 0.9504\n",
      "Epoch 77/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1174 - acc: 0.9493\n",
      "Epoch 78/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.1227 - acc: 0.9485\n",
      "Epoch 79/200\n",
      "5420/5420 [==============================] - 2s 292us/step - loss: 0.1130 - acc: 0.9537\n",
      "Epoch 80/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.1174 - acc: 0.9489\n",
      "Epoch 81/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.1042 - acc: 0.9565\n",
      "Epoch 82/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1070 - acc: 0.9542\n",
      "Epoch 83/200\n",
      "5420/5420 [==============================] - 2s 305us/step - loss: 0.1061 - acc: 0.9557\n",
      "Epoch 84/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.1080 - acc: 0.9559\n",
      "Epoch 85/200\n",
      "5420/5420 [==============================] - 2s 344us/step - loss: 0.1069 - acc: 0.9566\n",
      "Epoch 86/200\n",
      "5420/5420 [==============================] - 2s 322us/step - loss: 0.1009 - acc: 0.9581\n",
      "Epoch 87/200\n",
      "5420/5420 [==============================] - 2s 319us/step - loss: 0.1029 - acc: 0.9544\n",
      "Epoch 88/200\n",
      "5420/5420 [==============================] - 2s 321us/step - loss: 0.1023 - acc: 0.9598\n",
      "Epoch 89/200\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.1158 - acc: 0.9546\n",
      "Epoch 90/200\n",
      "5420/5420 [==============================] - 2s 319us/step - loss: 0.1048 - acc: 0.9555\n",
      "Epoch 91/200\n",
      "5420/5420 [==============================] - 2s 322us/step - loss: 0.0991 - acc: 0.9581\n",
      "Epoch 92/200\n",
      "5420/5420 [==============================] - 2s 330us/step - loss: 0.0997 - acc: 0.9590\n",
      "Epoch 93/200\n",
      "5420/5420 [==============================] - 2s 301us/step - loss: 0.0990 - acc: 0.9607\n",
      "Epoch 94/200\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.0979 - acc: 0.9622\n",
      "Epoch 95/200\n",
      "5420/5420 [==============================] - 2s 302us/step - loss: 0.1099 - acc: 0.9598\n",
      "Epoch 96/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.1008 - acc: 0.9594\n",
      "Epoch 97/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0962 - acc: 0.9592\n",
      "Epoch 98/200\n",
      "5420/5420 [==============================] - 2s 303us/step - loss: 0.0969 - acc: 0.9633\n",
      "Epoch 99/200\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.0865 - acc: 0.9646\n",
      "Epoch 100/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0841 - acc: 0.9661\n",
      "Epoch 101/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0883 - acc: 0.9659\n",
      "Epoch 102/200\n",
      "5420/5420 [==============================] - 2s 295us/step - loss: 0.0877 - acc: 0.9631\n",
      "Epoch 103/200\n",
      "5420/5420 [==============================] - 2s 303us/step - loss: 0.0898 - acc: 0.9635\n",
      "Epoch 104/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.0854 - acc: 0.9640\n",
      "Epoch 105/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0859 - acc: 0.9666\n",
      "Epoch 106/200\n",
      "5420/5420 [==============================] - 2s 344us/step - loss: 0.0829 - acc: 0.9655\n",
      "Epoch 107/200\n",
      "5420/5420 [==============================] - 2s 352us/step - loss: 0.0787 - acc: 0.9677\n",
      "Epoch 108/200\n",
      "5420/5420 [==============================] - 2s 336us/step - loss: 0.0795 - acc: 0.9685\n",
      "Epoch 109/200\n",
      "5420/5420 [==============================] - 2s 343us/step - loss: 0.0748 - acc: 0.9705\n",
      "Epoch 110/200\n",
      "5420/5420 [==============================] - 2s 327us/step - loss: 0.0931 - acc: 0.9627\n",
      "Epoch 111/200\n",
      "5420/5420 [==============================] - 2s 317us/step - loss: 0.1008 - acc: 0.9603\n",
      "Epoch 112/200\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.0886 - acc: 0.9635\n",
      "Epoch 113/200\n",
      "5420/5420 [==============================] - 2s 359us/step - loss: 0.0779 - acc: 0.9655\n",
      "Epoch 114/200\n",
      "5420/5420 [==============================] - 2s 359us/step - loss: 0.0825 - acc: 0.9653\n",
      "Epoch 115/200\n",
      "5420/5420 [==============================] - 2s 345us/step - loss: 0.0804 - acc: 0.9688\n",
      "Epoch 116/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.0829 - acc: 0.9672\n",
      "Epoch 117/200\n",
      "5420/5420 [==============================] - 2s 312us/step - loss: 0.0730 - acc: 0.9686\n",
      "Epoch 118/200\n",
      "5420/5420 [==============================] - 2s 313us/step - loss: 0.0702 - acc: 0.9701\n",
      "Epoch 119/200\n",
      "5420/5420 [==============================] - 2s 330us/step - loss: 0.0675 - acc: 0.9736\n",
      "Epoch 120/200\n",
      "5420/5420 [==============================] - 2s 314us/step - loss: 0.0698 - acc: 0.9705\n",
      "Epoch 121/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0688 - acc: 0.9720\n",
      "Epoch 122/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0607 - acc: 0.9738\n",
      "Epoch 123/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0602 - acc: 0.9734\n",
      "Epoch 124/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0657 - acc: 0.9712\n",
      "Epoch 125/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0794 - acc: 0.9694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0747 - acc: 0.9696\n",
      "Epoch 127/200\n",
      "5420/5420 [==============================] - 2s 312us/step - loss: 0.0759 - acc: 0.9697\n",
      "Epoch 128/200\n",
      "5420/5420 [==============================] - 2s 301us/step - loss: 0.0700 - acc: 0.9710\n",
      "Epoch 129/200\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.0629 - acc: 0.9753\n",
      "Epoch 130/200\n",
      "5420/5420 [==============================] - 2s 325us/step - loss: 0.0675 - acc: 0.9731\n",
      "Epoch 131/200\n",
      "5420/5420 [==============================] - 2s 363us/step - loss: 0.0660 - acc: 0.9740\n",
      "Epoch 132/200\n",
      "5420/5420 [==============================] - 2s 361us/step - loss: 0.0739 - acc: 0.9710\n",
      "Epoch 133/200\n",
      "5420/5420 [==============================] - 2s 376us/step - loss: 0.0742 - acc: 0.9734\n",
      "Epoch 134/200\n",
      "5420/5420 [==============================] - 2s 376us/step - loss: 0.0683 - acc: 0.9716\n",
      "Epoch 135/200\n",
      "5420/5420 [==============================] - 2s 344us/step - loss: 0.0619 - acc: 0.9753\n",
      "Epoch 136/200\n",
      "5420/5420 [==============================] - 2s 328us/step - loss: 0.0660 - acc: 0.9758\n",
      "Epoch 137/200\n",
      "5420/5420 [==============================] - 2s 331us/step - loss: 0.0650 - acc: 0.9755\n",
      "Epoch 138/200\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.0654 - acc: 0.9745\n",
      "Epoch 139/200\n",
      "5420/5420 [==============================] - 2s 323us/step - loss: 0.0592 - acc: 0.9780\n",
      "Epoch 140/200\n",
      "5420/5420 [==============================] - 2s 333us/step - loss: 0.0634 - acc: 0.9775\n",
      "Epoch 141/200\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.0700 - acc: 0.9742\n",
      "Epoch 142/200\n",
      "5420/5420 [==============================] - 2s 345us/step - loss: 0.0629 - acc: 0.9734\n",
      "Epoch 143/200\n",
      "5420/5420 [==============================] - 2s 363us/step - loss: 0.0594 - acc: 0.9749\n",
      "Epoch 144/200\n",
      "5420/5420 [==============================] - 2s 375us/step - loss: 0.0637 - acc: 0.9758\n",
      "Epoch 145/200\n",
      "5420/5420 [==============================] - 2s 391us/step - loss: 0.0611 - acc: 0.9758\n",
      "Epoch 146/200\n",
      "5420/5420 [==============================] - 2s 352us/step - loss: 0.0659 - acc: 0.9744\n",
      "Epoch 147/200\n",
      "5420/5420 [==============================] - 2s 354us/step - loss: 0.0633 - acc: 0.9747\n",
      "Epoch 148/200\n",
      "5420/5420 [==============================] - 2s 385us/step - loss: 0.0608 - acc: 0.9758\n",
      "Epoch 149/200\n",
      "5420/5420 [==============================] - 2s 325us/step - loss: 0.0579 - acc: 0.9762\n",
      "Epoch 150/200\n",
      "5420/5420 [==============================] - 2s 357us/step - loss: 0.0598 - acc: 0.9755\n",
      "Epoch 151/200\n",
      "5420/5420 [==============================] - 2s 359us/step - loss: 0.0550 - acc: 0.9777\n",
      "Epoch 152/200\n",
      "5420/5420 [==============================] - 2s 344us/step - loss: 0.0511 - acc: 0.9788\n",
      "Epoch 153/200\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.0594 - acc: 0.9751\n",
      "Epoch 154/200\n",
      "5420/5420 [==============================] - 2s 326us/step - loss: 0.0525 - acc: 0.9782\n",
      "Epoch 155/200\n",
      "5420/5420 [==============================] - 2s 312us/step - loss: 0.0656 - acc: 0.9738\n",
      "Epoch 156/200\n",
      "5420/5420 [==============================] - 2s 349us/step - loss: 0.0628 - acc: 0.9747\n",
      "Epoch 157/200\n",
      "5420/5420 [==============================] - 2s 378us/step - loss: 0.0531 - acc: 0.9797\n",
      "Epoch 158/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.0507 - acc: 0.9801\n",
      "Epoch 159/200\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.0510 - acc: 0.9801\n",
      "Epoch 160/200\n",
      "5420/5420 [==============================] - 2s 333us/step - loss: 0.0589 - acc: 0.9749\n",
      "Epoch 161/200\n",
      "5420/5420 [==============================] - 2s 343us/step - loss: 0.0547 - acc: 0.9780\n",
      "Epoch 162/200\n",
      "5420/5420 [==============================] - 2s 358us/step - loss: 0.0582 - acc: 0.9775\n",
      "Epoch 163/200\n",
      "5420/5420 [==============================] - 2s 326us/step - loss: 0.0533 - acc: 0.9792\n",
      "Epoch 164/200\n",
      "5420/5420 [==============================] - 2s 327us/step - loss: 0.0516 - acc: 0.9806\n",
      "Epoch 165/200\n",
      "5420/5420 [==============================] - 2s 348us/step - loss: 0.0478 - acc: 0.9788\n",
      "Epoch 166/200\n",
      "5420/5420 [==============================] - 2s 379us/step - loss: 0.0411 - acc: 0.9810\n",
      "Epoch 167/200\n",
      "5420/5420 [==============================] - 2s 344us/step - loss: 0.0527 - acc: 0.9788\n",
      "Epoch 168/200\n",
      "5420/5420 [==============================] - 2s 343us/step - loss: 0.0567 - acc: 0.9768\n",
      "Epoch 169/200\n",
      "5420/5420 [==============================] - 2s 343us/step - loss: 0.0531 - acc: 0.9784\n",
      "Epoch 170/200\n",
      "5420/5420 [==============================] - 2s 357us/step - loss: 0.0489 - acc: 0.9817\n",
      "Epoch 171/200\n",
      "5420/5420 [==============================] - 2s 385us/step - loss: 0.0511 - acc: 0.9803\n",
      "Epoch 172/200\n",
      "5420/5420 [==============================] - 2s 364us/step - loss: 0.0493 - acc: 0.9786\n",
      "Epoch 173/200\n",
      "5420/5420 [==============================] - 2s 338us/step - loss: 0.0449 - acc: 0.9817\n",
      "Epoch 174/200\n",
      "5420/5420 [==============================] - 2s 324us/step - loss: 0.0442 - acc: 0.9817\n",
      "Epoch 175/200\n",
      "5420/5420 [==============================] - 2s 346us/step - loss: 0.0485 - acc: 0.9810\n",
      "Epoch 176/200\n",
      "5420/5420 [==============================] - 2s 302us/step - loss: 0.0573 - acc: 0.9771\n",
      "Epoch 177/200\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.0527 - acc: 0.9782\n",
      "Epoch 178/200\n",
      "5420/5420 [==============================] - 2s 335us/step - loss: 0.0562 - acc: 0.9784\n",
      "Epoch 179/200\n",
      "5420/5420 [==============================] - 2s 314us/step - loss: 0.0449 - acc: 0.9804\n",
      "Epoch 180/200\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.0462 - acc: 0.9810\n",
      "Epoch 181/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0500 - acc: 0.9817\n",
      "Epoch 182/200\n",
      "5420/5420 [==============================] - 2s 316us/step - loss: 0.0527 - acc: 0.9782\n",
      "Epoch 183/200\n",
      "5420/5420 [==============================] - 2s 353us/step - loss: 0.0556 - acc: 0.9777\n",
      "Epoch 184/200\n",
      "5420/5420 [==============================] - 2s 355us/step - loss: 0.0505 - acc: 0.9801\n",
      "Epoch 185/200\n",
      "5420/5420 [==============================] - 2s 300us/step - loss: 0.0473 - acc: 0.9810\n",
      "Epoch 186/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0463 - acc: 0.9804\n",
      "Epoch 187/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0516 - acc: 0.9779\n",
      "Epoch 188/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0561 - acc: 0.9771\n",
      "Epoch 189/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0557 - acc: 0.9790\n",
      "Epoch 190/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0507 - acc: 0.9804\n",
      "Epoch 191/200\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.0445 - acc: 0.9806\n",
      "Epoch 192/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0395 - acc: 0.9849\n",
      "Epoch 193/200\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.0432 - acc: 0.9830\n",
      "Epoch 194/200\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.0428 - acc: 0.9821\n",
      "Epoch 195/200\n",
      "5420/5420 [==============================] - 2s 307us/step - loss: 0.0457 - acc: 0.9812\n",
      "Epoch 196/200\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0449 - acc: 0.9828\n",
      "Epoch 197/200\n",
      "5420/5420 [==============================] - 2s 303us/step - loss: 0.0408 - acc: 0.9838\n",
      "Epoch 198/200\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.0441 - acc: 0.9821\n",
      "Epoch 199/200\n",
      "5420/5420 [==============================] - 2s 339us/step - loss: 0.0418 - acc: 0.9838\n",
      "Epoch 200/200\n",
      "5420/5420 [==============================] - 2s 304us/step - loss: 0.0330 - acc: 0.9878\n",
      "Accuracy: 0.733298\n",
      "F1-score: 0.793385\n",
      "F2-macro: 0.694226\n",
      "CPU times: user 3min 30s, sys: 1min 13s, total: 4min 44s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inp1 = Input(shape=(embedded_data.shape[1],))\n",
    "inp2 = Input(shape=(embedded_data.shape[1],))\n",
    "\n",
    "dense12 = Dense(2048, activation='relu')(inp1)\n",
    "dense22 = Dense(2048, activation='relu')(inp2)\n",
    "\n",
    "dp1 = Dropout(0.35)(dense12)\n",
    "dp2 = Dropout(0.35)(dense22)\n",
    "\n",
    "dense13 = Dense(2048, activation='relu')(dp1)\n",
    "dense23 = Dense(2048, activation='relu')(dp2)\n",
    "\n",
    "dp11 = Dropout(0.3)(dense13)\n",
    "dp22 = Dropout(0.3)(dense23)\n",
    "\n",
    "mrg = Average()([dp11, dp22])\n",
    "\n",
    "dense3 = Dense(1024, activation='relu')(mrg)\n",
    "dp33 = Dropout(0.3)(dense3)\n",
    "dense4 = Dense(1024, activation='relu')(dp33)\n",
    "dp44 = Dropout(0.3)(dense4)\n",
    "res_dense = Dense(1, activation='sigmoid')(dp44)\n",
    "\n",
    "model1 = Model(input=[inp1, inp2], output=res_dense)\n",
    "adam = Adam(lr=0.0005)\n",
    "model1.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model1.summary())\n",
    "# fit the model\n",
    "history1 = model1.fit([texts_1, texts_2], y_train, epochs=200, batch_size=512, verbose=1)\n",
    "# # evaluate\n",
    "y_pred = model1.predict([texts_1_test, texts_2_test]).flatten().round()\n",
    "print('Accuracy: %f' % (accuracy_score(y_test, y_pred)))\n",
    "print('F1-score: %f' % (f1_score(y_test, y_pred)))\n",
    "print('F2-macro: %f' % (fbeta_score(y_test, y_pred, beta=2, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~30mb1/34.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter(\n",
    "        x=list(range(200)),\n",
    "        y=history1.history['acc'],\n",
    "        name='Accuracy'\n",
    "    )\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "        x=list(range(200)),\n",
    "        y=history1.history['loss'],\n",
    "        name='Loss'\n",
    "    )\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Training history visualization',\n",
    "              xaxis = dict(title = 'Epoch')\n",
    "              )\n",
    "\n",
    "fig = dict(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "# fig['layout'].update(height=1000, width=1000, title='Embeddings & Scores')\n",
    "py.iplot(fig, filename='Siamese training history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка, если график не прогружается](https://plot.ly/~30mb1/34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 23, 100)           982600    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2300)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              9424896   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 20,897,353\n",
      "Trainable params: 19,914,753\n",
      "Non-trainable params: 982,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "5420/5420 [==============================] - 4s 764us/step - loss: 0.8456 - acc: 0.5476\n",
      "Epoch 2/50\n",
      "5420/5420 [==============================] - 2s 458us/step - loss: 0.6693 - acc: 0.6384\n",
      "Epoch 3/50\n",
      "5420/5420 [==============================] - 2s 454us/step - loss: 0.6425 - acc: 0.6600\n",
      "Epoch 4/50\n",
      "5420/5420 [==============================] - 2s 420us/step - loss: 0.6147 - acc: 0.6779 1s - loss: 0.6187 - ac\n",
      "Epoch 5/50\n",
      "5420/5420 [==============================] - 2s 430us/step - loss: 0.5852 - acc: 0.7055\n",
      "Epoch 6/50\n",
      "5420/5420 [==============================] - 2s 441us/step - loss: 0.5569 - acc: 0.7271\n",
      "Epoch 7/50\n",
      "5420/5420 [==============================] - 2s 443us/step - loss: 0.5195 - acc: 0.7561\n",
      "Epoch 8/50\n",
      "5420/5420 [==============================] - 2s 422us/step - loss: 0.4478 - acc: 0.7965\n",
      "Epoch 9/50\n",
      "5420/5420 [==============================] - 2s 424us/step - loss: 0.3792 - acc: 0.8391\n",
      "Epoch 10/50\n",
      "5420/5420 [==============================] - 3s 464us/step - loss: 0.3064 - acc: 0.8725\n",
      "Epoch 11/50\n",
      "5420/5420 [==============================] - 3s 490us/step - loss: 0.2584 - acc: 0.9022\n",
      "Epoch 12/50\n",
      "5420/5420 [==============================] - 3s 477us/step - loss: 0.2135 - acc: 0.9229\n",
      "Epoch 13/50\n",
      "5420/5420 [==============================] - 3s 516us/step - loss: 0.1889 - acc: 0.9321\n",
      "Epoch 14/50\n",
      "5420/5420 [==============================] - 2s 428us/step - loss: 0.1569 - acc: 0.9472\n",
      "Epoch 15/50\n",
      "5420/5420 [==============================] - 2s 439us/step - loss: 0.1551 - acc: 0.9450\n",
      "Epoch 16/50\n",
      "5420/5420 [==============================] - 2s 424us/step - loss: 0.1348 - acc: 0.9530\n",
      "Epoch 17/50\n",
      "5420/5420 [==============================] - 2s 436us/step - loss: 0.1045 - acc: 0.9629\n",
      "Epoch 18/50\n",
      "5420/5420 [==============================] - 2s 412us/step - loss: 0.0992 - acc: 0.9642\n",
      "Epoch 19/50\n",
      "5420/5420 [==============================] - 2s 428us/step - loss: 0.1015 - acc: 0.9672\n",
      "Epoch 20/50\n",
      "5420/5420 [==============================] - 2s 422us/step - loss: 0.0908 - acc: 0.9701\n",
      "Epoch 21/50\n",
      "5420/5420 [==============================] - 3s 474us/step - loss: 0.0733 - acc: 0.9732\n",
      "Epoch 22/50\n",
      "5420/5420 [==============================] - 2s 426us/step - loss: 0.0660 - acc: 0.9777\n",
      "Epoch 23/50\n",
      "5420/5420 [==============================] - 2s 439us/step - loss: 0.0759 - acc: 0.9734\n",
      "Epoch 24/50\n",
      "5420/5420 [==============================] - 3s 463us/step - loss: 0.0734 - acc: 0.9762\n",
      "Epoch 25/50\n",
      "5420/5420 [==============================] - 2s 427us/step - loss: 0.0739 - acc: 0.9753\n",
      "Epoch 26/50\n",
      "5420/5420 [==============================] - 2s 415us/step - loss: 0.0674 - acc: 0.9786\n",
      "Epoch 27/50\n",
      "5420/5420 [==============================] - 2s 411us/step - loss: 0.0531 - acc: 0.9827\n",
      "Epoch 28/50\n",
      "5420/5420 [==============================] - 2s 408us/step - loss: 0.0612 - acc: 0.9812\n",
      "Epoch 29/50\n",
      "5420/5420 [==============================] - 2s 406us/step - loss: 0.0670 - acc: 0.9775\n",
      "Epoch 30/50\n",
      "5420/5420 [==============================] - 2s 404us/step - loss: 0.0533 - acc: 0.9819\n",
      "Epoch 31/50\n",
      "5420/5420 [==============================] - 2s 407us/step - loss: 0.0478 - acc: 0.9849\n",
      "Epoch 32/50\n",
      "5420/5420 [==============================] - 2s 406us/step - loss: 0.0340 - acc: 0.9884\n",
      "Epoch 33/50\n",
      "5420/5420 [==============================] - 2s 407us/step - loss: 0.0454 - acc: 0.9839\n",
      "Epoch 34/50\n",
      "5420/5420 [==============================] - 2s 406us/step - loss: 0.0411 - acc: 0.9863\n",
      "Epoch 35/50\n",
      "5420/5420 [==============================] - 2s 406us/step - loss: 0.0342 - acc: 0.9876\n",
      "Epoch 36/50\n",
      "5420/5420 [==============================] - 2s 408us/step - loss: 0.0319 - acc: 0.9899\n",
      "Epoch 37/50\n",
      "5420/5420 [==============================] - 2s 440us/step - loss: 0.0381 - acc: 0.9869\n",
      "Epoch 38/50\n",
      "5420/5420 [==============================] - 2s 439us/step - loss: 0.0322 - acc: 0.9895\n",
      "Epoch 39/50\n",
      "5420/5420 [==============================] - 2s 415us/step - loss: 0.0334 - acc: 0.9875\n",
      "Epoch 40/50\n",
      "5420/5420 [==============================] - 2s 415us/step - loss: 0.0318 - acc: 0.9895\n",
      "Epoch 41/50\n",
      "5420/5420 [==============================] - 3s 497us/step - loss: 0.0297 - acc: 0.9880\n",
      "Epoch 42/50\n",
      "5420/5420 [==============================] - 2s 424us/step - loss: 0.0394 - acc: 0.9875\n",
      "Epoch 43/50\n",
      "5420/5420 [==============================] - 2s 421us/step - loss: 0.0350 - acc: 0.9882\n",
      "Epoch 44/50\n",
      "5420/5420 [==============================] - 2s 409us/step - loss: 0.0313 - acc: 0.9887\n",
      "Epoch 45/50\n",
      "5420/5420 [==============================] - 2s 412us/step - loss: 0.0410 - acc: 0.9875\n",
      "Epoch 46/50\n",
      "5420/5420 [==============================] - 2s 409us/step - loss: 0.0338 - acc: 0.9862 0s - loss: 0.0343 - acc: 0.98\n",
      "Epoch 47/50\n",
      "5420/5420 [==============================] - 2s 413us/step - loss: 0.0350 - acc: 0.9878\n",
      "Epoch 48/50\n",
      "5420/5420 [==============================] - 2s 425us/step - loss: 0.0277 - acc: 0.9906\n",
      "Epoch 49/50\n",
      "5420/5420 [==============================] - 2s 412us/step - loss: 0.0320 - acc: 0.9882\n",
      "Epoch 50/50\n",
      "5420/5420 [==============================] - 2s 409us/step - loss: 0.0262 - acc: 0.9908\n",
      "Accuracy: 0.694521\n",
      "F1-score: 0.771712\n",
      "F2-macro: 0.651641\n",
      "CPU times: user 1min 13s, sys: 24.5 s, total: 1min 38s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model2 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model2.add(e)\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(4096, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(2048, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "adam = Adam(lr=0.0005)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model2.summary())\n",
    "# fit the model\n",
    "history2 = model2.fit(padded_texts_train, y_train, epochs=50, batch_size=512, verbose=1)\n",
    "# evaluate the model\n",
    "y_pred = model2.predict_classes(padded_texts_test).flatten()\n",
    "print('Accuracy: %f' % (accuracy_score(y_test, y_pred)))\n",
    "print('F1-score: %f' % (f1_score(y_test, y_pred)))\n",
    "print('F2-macro: %f' % (fbeta_score(y_test, y_pred, beta=2, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~30mb1/38.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter(\n",
    "        x=list(range(100)),\n",
    "        y=history2.history['acc'],\n",
    "        name='Accuracy'\n",
    "    )\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "        x=list(range(100)),\n",
    "        y=history2.history['loss'],\n",
    "        name='Loss'\n",
    "    )\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Training history visualization',\n",
    "              xaxis = dict(title = 'Epoch')\n",
    "              )\n",
    "\n",
    "fig = dict(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Embedding-layred training history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка, если график не прогружается](https://plot.ly/~30mb1/38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 10,903,553\n",
      "Trainable params: 10,903,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "5420/5420 [==============================] - 3s 491us/step - loss: 0.6701 - acc: 0.6295\n",
      "Epoch 2/150\n",
      "5420/5420 [==============================] - 1s 272us/step - loss: 0.6345 - acc: 0.6594\n",
      "Epoch 3/150\n",
      "5420/5420 [==============================] - 1s 276us/step - loss: 0.6120 - acc: 0.6766\n",
      "Epoch 4/150\n",
      "5420/5420 [==============================] - 2s 311us/step - loss: 0.6003 - acc: 0.6884\n",
      "Epoch 5/150\n",
      "5420/5420 [==============================] - 2s 279us/step - loss: 0.5932 - acc: 0.6983\n",
      "Epoch 6/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.5862 - acc: 0.6991\n",
      "Epoch 7/150\n",
      "5420/5420 [==============================] - 2s 280us/step - loss: 0.5757 - acc: 0.7111\n",
      "Epoch 8/150\n",
      "5420/5420 [==============================] - 1s 256us/step - loss: 0.5760 - acc: 0.7035\n",
      "Epoch 9/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.5602 - acc: 0.7124\n",
      "Epoch 10/150\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.5527 - acc: 0.7216\n",
      "Epoch 11/150\n",
      "5420/5420 [==============================] - 2s 301us/step - loss: 0.5409 - acc: 0.7280\n",
      "Epoch 12/150\n",
      "5420/5420 [==============================] - 2s 313us/step - loss: 0.5401 - acc: 0.7280\n",
      "Epoch 13/150\n",
      "5420/5420 [==============================] - 2s 288us/step - loss: 0.5395 - acc: 0.7321\n",
      "Epoch 14/150\n",
      "5420/5420 [==============================] - 2s 323us/step - loss: 0.5333 - acc: 0.7345\n",
      "Epoch 15/150\n",
      "5420/5420 [==============================] - 2s 359us/step - loss: 0.5164 - acc: 0.7517\n",
      "Epoch 16/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.5065 - acc: 0.7570\n",
      "Epoch 17/150\n",
      "5420/5420 [==============================] - 1s 257us/step - loss: 0.5045 - acc: 0.7544\n",
      "Epoch 18/150\n",
      "5420/5420 [==============================] - 1s 273us/step - loss: 0.4993 - acc: 0.7559\n",
      "Epoch 19/150\n",
      "5420/5420 [==============================] - 1s 256us/step - loss: 0.5004 - acc: 0.7511\n",
      "Epoch 20/150\n",
      "5420/5420 [==============================] - 1s 255us/step - loss: 0.4853 - acc: 0.7690\n",
      "Epoch 21/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.4844 - acc: 0.7585\n",
      "Epoch 22/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.4793 - acc: 0.7714\n",
      "Epoch 23/150\n",
      "5420/5420 [==============================] - 1s 270us/step - loss: 0.4618 - acc: 0.7705\n",
      "Epoch 24/150\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.4664 - acc: 0.7779\n",
      "Epoch 25/150\n",
      "5420/5420 [==============================] - 1s 273us/step - loss: 0.4741 - acc: 0.7714\n",
      "Epoch 26/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.4469 - acc: 0.7869\n",
      "Epoch 27/150\n",
      "5420/5420 [==============================] - 2s 300us/step - loss: 0.4363 - acc: 0.7980\n",
      "Epoch 28/150\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.4383 - acc: 0.7950\n",
      "Epoch 29/150\n",
      "5420/5420 [==============================] - 2s 338us/step - loss: 0.4344 - acc: 0.7956\n",
      "Epoch 30/150\n",
      "5420/5420 [==============================] - 2s 319us/step - loss: 0.4324 - acc: 0.7945\n",
      "Epoch 31/150\n",
      "5420/5420 [==============================] - 2s 302us/step - loss: 0.4107 - acc: 0.8129\n",
      "Epoch 32/150\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.4057 - acc: 0.8096\n",
      "Epoch 33/150\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.3955 - acc: 0.8122\n",
      "Epoch 34/150\n",
      "5420/5420 [==============================] - 2s 326us/step - loss: 0.3926 - acc: 0.8192\n",
      "Epoch 35/150\n",
      "5420/5420 [==============================] - 2s 321us/step - loss: 0.3858 - acc: 0.8210\n",
      "Epoch 36/150\n",
      "5420/5420 [==============================] - 2s 320us/step - loss: 0.3849 - acc: 0.8164\n",
      "Epoch 37/150\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.3761 - acc: 0.8280\n",
      "Epoch 38/150\n",
      "5420/5420 [==============================] - 1s 268us/step - loss: 0.3672 - acc: 0.8292\n",
      "Epoch 39/150\n",
      "5420/5420 [==============================] - 2s 314us/step - loss: 0.3779 - acc: 0.8256\n",
      "Epoch 40/150\n",
      "5420/5420 [==============================] - 2s 305us/step - loss: 0.3611 - acc: 0.8321\n",
      "Epoch 41/150\n",
      "5420/5420 [==============================] - 2s 313us/step - loss: 0.3567 - acc: 0.8376\n",
      "Epoch 42/150\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.3632 - acc: 0.8299\n",
      "Epoch 43/150\n",
      "5420/5420 [==============================] - 2s 340us/step - loss: 0.3502 - acc: 0.8417\n",
      "Epoch 44/150\n",
      "5420/5420 [==============================] - 2s 283us/step - loss: 0.3197 - acc: 0.8539\n",
      "Epoch 45/150\n",
      "5420/5420 [==============================] - 2s 283us/step - loss: 0.3071 - acc: 0.8587\n",
      "Epoch 46/150\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.3050 - acc: 0.8594\n",
      "Epoch 47/150\n",
      "5420/5420 [==============================] - 2s 280us/step - loss: 0.3153 - acc: 0.8592\n",
      "Epoch 48/150\n",
      "5420/5420 [==============================] - 2s 289us/step - loss: 0.3176 - acc: 0.8565\n",
      "Epoch 49/150\n",
      "5420/5420 [==============================] - 2s 285us/step - loss: 0.3171 - acc: 0.8554\n",
      "Epoch 50/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.2947 - acc: 0.8661\n",
      "Epoch 51/150\n",
      "5420/5420 [==============================] - 2s 278us/step - loss: 0.2911 - acc: 0.8677\n",
      "Epoch 52/150\n",
      "5420/5420 [==============================] - 2s 284us/step - loss: 0.2865 - acc: 0.8672\n",
      "Epoch 53/150\n",
      "5420/5420 [==============================] - 1s 272us/step - loss: 0.2802 - acc: 0.8775\n",
      "Epoch 54/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.2794 - acc: 0.8729\n",
      "Epoch 55/150\n",
      "5420/5420 [==============================] - 1s 266us/step - loss: 0.2796 - acc: 0.8736\n",
      "Epoch 56/150\n",
      "5420/5420 [==============================] - 1s 271us/step - loss: 0.2580 - acc: 0.8856\n",
      "Epoch 57/150\n",
      "5420/5420 [==============================] - 2s 332us/step - loss: 0.2561 - acc: 0.8863\n",
      "Epoch 58/150\n",
      "5420/5420 [==============================] - 1s 272us/step - loss: 0.2551 - acc: 0.8884\n",
      "Epoch 59/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.2506 - acc: 0.8917\n",
      "Epoch 60/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.2482 - acc: 0.8827\n",
      "Epoch 61/150\n",
      "5420/5420 [==============================] - 1s 256us/step - loss: 0.2385 - acc: 0.8961\n",
      "Epoch 62/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.2250 - acc: 0.8996\n",
      "Epoch 63/150\n",
      "5420/5420 [==============================] - 1s 262us/step - loss: 0.2282 - acc: 0.9013\n",
      "Epoch 64/150\n",
      "5420/5420 [==============================] - 1s 254us/step - loss: 0.2289 - acc: 0.8996\n",
      "Epoch 65/150\n",
      "5420/5420 [==============================] - 1s 254us/step - loss: 0.2443 - acc: 0.8926\n",
      "Epoch 66/150\n",
      "5420/5420 [==============================] - 1s 254us/step - loss: 0.2399 - acc: 0.8983\n",
      "Epoch 67/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.2709 - acc: 0.8797\n",
      "Epoch 68/150\n",
      "5420/5420 [==============================] - 1s 252us/step - loss: 0.2344 - acc: 0.8969\n",
      "Epoch 69/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.2173 - acc: 0.9055\n",
      "Epoch 70/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.1974 - acc: 0.9129\n",
      "Epoch 71/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.1914 - acc: 0.9188\n",
      "Epoch 72/150\n",
      "5420/5420 [==============================] - 1s 275us/step - loss: 0.1861 - acc: 0.9244\n",
      "Epoch 73/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1834 - acc: 0.9201\n",
      "Epoch 74/150\n",
      "5420/5420 [==============================] - 1s 253us/step - loss: 0.1855 - acc: 0.9201\n",
      "Epoch 75/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.1773 - acc: 0.9203\n",
      "Epoch 76/150\n",
      "5420/5420 [==============================] - 1s 264us/step - loss: 0.1747 - acc: 0.9277\n",
      "Epoch 77/150\n",
      "5420/5420 [==============================] - 2s 293us/step - loss: 0.1885 - acc: 0.9177\n",
      "Epoch 78/150\n",
      "5420/5420 [==============================] - 2s 308us/step - loss: 0.1768 - acc: 0.9234\n",
      "Epoch 79/150\n",
      "5420/5420 [==============================] - 2s 304us/step - loss: 0.1736 - acc: 0.9262\n",
      "Epoch 80/150\n",
      "5420/5420 [==============================] - 2s 298us/step - loss: 0.1783 - acc: 0.9208\n",
      "Epoch 81/150\n",
      "5420/5420 [==============================] - 2s 282us/step - loss: 0.1782 - acc: 0.9197\n",
      "Epoch 82/150\n",
      "5420/5420 [==============================] - 1s 271us/step - loss: 0.1759 - acc: 0.9282\n",
      "Epoch 83/150\n",
      "5420/5420 [==============================] - 1s 256us/step - loss: 0.1579 - acc: 0.9352\n",
      "Epoch 84/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1549 - acc: 0.9362\n",
      "Epoch 85/150\n",
      "5420/5420 [==============================] - 1s 255us/step - loss: 0.1567 - acc: 0.9332\n",
      "Epoch 86/150\n",
      "5420/5420 [==============================] - 1s 269us/step - loss: 0.1729 - acc: 0.9199\n",
      "Epoch 87/150\n",
      "5420/5420 [==============================] - 2s 287us/step - loss: 0.1813 - acc: 0.9196\n",
      "Epoch 88/150\n",
      "5420/5420 [==============================] - 1s 276us/step - loss: 0.1641 - acc: 0.9299\n",
      "Epoch 89/150\n",
      "5420/5420 [==============================] - 1s 272us/step - loss: 0.1643 - acc: 0.9282\n",
      "Epoch 90/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.1575 - acc: 0.9290\n",
      "Epoch 91/150\n",
      "5420/5420 [==============================] - 1s 270us/step - loss: 0.1480 - acc: 0.9356\n",
      "Epoch 92/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.1452 - acc: 0.9365\n",
      "Epoch 93/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.1347 - acc: 0.9437\n",
      "Epoch 94/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.1343 - acc: 0.9445\n",
      "Epoch 95/150\n",
      "5420/5420 [==============================] - 1s 254us/step - loss: 0.1391 - acc: 0.9397\n",
      "Epoch 96/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.1337 - acc: 0.9434\n",
      "Epoch 97/150\n",
      "5420/5420 [==============================] - 1s 274us/step - loss: 0.1317 - acc: 0.9448\n",
      "Epoch 98/150\n",
      "5420/5420 [==============================] - 1s 256us/step - loss: 0.1320 - acc: 0.9491\n",
      "Epoch 99/150\n",
      "5420/5420 [==============================] - 1s 270us/step - loss: 0.1299 - acc: 0.9439\n",
      "Epoch 100/150\n",
      "5420/5420 [==============================] - 2s 313us/step - loss: 0.1337 - acc: 0.9421\n",
      "Epoch 101/150\n",
      "5420/5420 [==============================] - 2s 334us/step - loss: 0.1182 - acc: 0.9500\n",
      "Epoch 102/150\n",
      "5420/5420 [==============================] - 2s 310us/step - loss: 0.1297 - acc: 0.9445\n",
      "Epoch 103/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1249 - acc: 0.9483\n",
      "Epoch 104/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.1322 - acc: 0.9421\n",
      "Epoch 105/150\n",
      "5420/5420 [==============================] - 1s 263us/step - loss: 0.1322 - acc: 0.9434\n",
      "Epoch 106/150\n",
      "5420/5420 [==============================] - 1s 262us/step - loss: 0.1328 - acc: 0.9432\n",
      "Epoch 107/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1263 - acc: 0.9465\n",
      "Epoch 108/150\n",
      "5420/5420 [==============================] - 1s 257us/step - loss: 0.1232 - acc: 0.9443\n",
      "Epoch 109/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.1222 - acc: 0.9472\n",
      "Epoch 110/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1192 - acc: 0.9474\n",
      "Epoch 111/150\n",
      "5420/5420 [==============================] - 2s 280us/step - loss: 0.1168 - acc: 0.9498\n",
      "Epoch 112/150\n",
      "5420/5420 [==============================] - 1s 277us/step - loss: 0.1145 - acc: 0.9504\n",
      "Epoch 113/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.1169 - acc: 0.9528\n",
      "Epoch 114/150\n",
      "5420/5420 [==============================] - 2s 294us/step - loss: 0.1182 - acc: 0.9522\n",
      "Epoch 115/150\n",
      "5420/5420 [==============================] - 2s 299us/step - loss: 0.1177 - acc: 0.9487\n",
      "Epoch 116/150\n",
      "5420/5420 [==============================] - 2s 277us/step - loss: 0.1130 - acc: 0.9526\n",
      "Epoch 117/150\n",
      "5420/5420 [==============================] - 2s 297us/step - loss: 0.1209 - acc: 0.9491\n",
      "Epoch 118/150\n",
      "5420/5420 [==============================] - 2s 315us/step - loss: 0.1122 - acc: 0.9530\n",
      "Epoch 119/150\n",
      "5420/5420 [==============================] - 2s 286us/step - loss: 0.1128 - acc: 0.9482\n",
      "Epoch 120/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.1114 - acc: 0.9524\n",
      "Epoch 121/150\n",
      "5420/5420 [==============================] - 2s 281us/step - loss: 0.1000 - acc: 0.9577\n",
      "Epoch 122/150\n",
      "5420/5420 [==============================] - 1s 268us/step - loss: 0.1128 - acc: 0.9542\n",
      "Epoch 123/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.1177 - acc: 0.9493\n",
      "Epoch 124/150\n",
      "5420/5420 [==============================] - 1s 264us/step - loss: 0.1088 - acc: 0.9550\n",
      "Epoch 125/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.1075 - acc: 0.9581\n",
      "Epoch 126/150\n",
      "5420/5420 [==============================] - 2s 277us/step - loss: 0.1089 - acc: 0.9520\n",
      "Epoch 127/150\n",
      "5420/5420 [==============================] - 1s 264us/step - loss: 0.1081 - acc: 0.9561\n",
      "Epoch 128/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.1041 - acc: 0.9539\n",
      "Epoch 129/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.1175 - acc: 0.9528\n",
      "Epoch 130/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.1131 - acc: 0.9509\n",
      "Epoch 131/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.1069 - acc: 0.9581\n",
      "Epoch 132/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.0967 - acc: 0.9601\n",
      "Epoch 133/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.0964 - acc: 0.9596\n",
      "Epoch 134/150\n",
      "5420/5420 [==============================] - 1s 257us/step - loss: 0.0835 - acc: 0.9648\n",
      "Epoch 135/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.0911 - acc: 0.9600\n",
      "Epoch 136/150\n",
      "5420/5420 [==============================] - 1s 258us/step - loss: 0.0892 - acc: 0.9642\n",
      "Epoch 137/150\n",
      "5420/5420 [==============================] - 1s 259us/step - loss: 0.0966 - acc: 0.9587\n",
      "Epoch 138/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.1001 - acc: 0.9563\n",
      "Epoch 139/150\n",
      "5420/5420 [==============================] - 1s 257us/step - loss: 0.0975 - acc: 0.9594\n",
      "Epoch 140/150\n",
      "5420/5420 [==============================] - 1s 266us/step - loss: 0.0925 - acc: 0.9601\n",
      "Epoch 141/150\n",
      "5420/5420 [==============================] - 2s 280us/step - loss: 0.0979 - acc: 0.9598\n",
      "Epoch 142/150\n",
      "5420/5420 [==============================] - 2s 289us/step - loss: 0.1009 - acc: 0.9537\n",
      "Epoch 143/150\n",
      "5420/5420 [==============================] - 2s 307us/step - loss: 0.0944 - acc: 0.9603\n",
      "Epoch 144/150\n",
      "5420/5420 [==============================] - 2s 287us/step - loss: 0.0960 - acc: 0.9601\n",
      "Epoch 145/150\n",
      "5420/5420 [==============================] - 1s 261us/step - loss: 0.0904 - acc: 0.9624\n",
      "Epoch 146/150\n",
      "5420/5420 [==============================] - 1s 265us/step - loss: 0.0944 - acc: 0.9620\n",
      "Epoch 147/150\n",
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.0913 - acc: 0.9611\n",
      "Epoch 148/150\n",
      "5420/5420 [==============================] - 1s 257us/step - loss: 0.1004 - acc: 0.9605\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5420/5420 [==============================] - 1s 260us/step - loss: 0.1051 - acc: 0.9596\n",
      "Epoch 150/150\n",
      "5420/5420 [==============================] - 2s 296us/step - loss: 0.0997 - acc: 0.9590\n",
      "Accuracy: 0.689579\n",
      "F1-score: 0.764349\n",
      "F2-macro: 0.651228\n",
      "CPU times: user 2min 15s, sys: 49.4 s, total: 3min 4s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(4096, input_dim=embedded_data.shape[1], activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(2048, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "adam = Adam(lr=0.0005)\n",
    "model3.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model3.summary())\n",
    "# fit the model\n",
    "history3 = model3.fit(embedded_data, y_train, epochs=150, batch_size=512, verbose=1)\n",
    "# evaluate the model\n",
    "y_pred = model3.predict_classes(embedded_data_test).flatten()\n",
    "print('Accuracy: %f' % (accuracy_score(y_test, y_pred)))\n",
    "print('F1-score: %f' % (f1_score(y_test, y_pred)))\n",
    "print('F2-macro: %f' % (fbeta_score(y_test, y_pred, beta=2, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~30mb1/40.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter(\n",
    "        x=list(range(150)),\n",
    "        y=history3.history['acc'],\n",
    "        name='Accuracy'\n",
    "    )\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "        x=list(range(150)),\n",
    "        y=history3.history['loss'],\n",
    "        name='Loss'\n",
    "    )\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Training history visualization',\n",
    "              xaxis = dict(title = 'Epoch')\n",
    "              )\n",
    "\n",
    "fig = dict(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Simple FNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка, если график не прогружается](https://plot.ly/~30mb1/40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сиамская нейросеть дала наилучшие результаты, однако ее время обучения было наибольшим из-за большого количества слоев. Далее с практически идентичными результатами шли простые нейросети, однако та, что использовала Embedding слой обучалась в разы быстрее (99% accuracy на тренировочный выборке при 50 эпохах против 96% на 150 эпохах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
